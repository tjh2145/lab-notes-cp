###10/20/14

##Notes before Hunting the Whale

- people don't use standard command-line tools for textual analysis, but it is possible

- Group for experimental methods in the humanities: rapid prototyping of speculative ideas 
- minimal viable citizen project: what are the minimal tools and practices you need to know to be a citizen in the digital world. Make a minimal viable citizen tutorial website - decide what are those tools and how to use them. Draft of tutorials. Touch-typing.
- Peer production and how to get involved in projects. How to create something online with 100+ people.
- Open source project. Think of your lab notebook as a product for public consumption.

##Hunting the Whale Exercise

`grep -w --file=humans.txt mobydick.txt `: this command identifies all of the words in humans.txt and finds them in mobydick.txt
- -w refers to the whole word, so that parts of the word will not be found in the text
- This command is pulling the entire text. Why? Because there is an empty space, so the command ispulling hte netire text with ever space.
- grep -wf (whole words in the file) 
- ELEGANT SOLUTION: grep -wf humans.txt mobydick.txt >> humanwords.txt 

###tokenizing: take each line and separate it into words

- sed 's/\s/\n/g' 
- take every space ( \ ) in a new line 
- sed syntax: \s = space, \n is new line, and create a fence ( / )
- take a space, substitute it with a new line, and do it globally
- DOESN'T WORK FOR MACS

###**SOLUTION for mac:** tokenizing
- sed -e 's/ /\'$'\n/g' animalcontext.txt  > animalcontexttoken.txt
- What does the $ mean?

###bash scripting
- think about the methodology, not what tools to use
- other languages: python, R (?)

####networking, accessing remote servers, version control, online cooperation, github

###cleaning up punctuation in the tokenized file
`cat your_file.txt | tr -d '[:punct:]' >> your_file_punctuation_removed.txt`

###removing spaces in a tokenized file

`sed ‘/ˆ$/d’ humantoken_nopunc.txt > humantokenclean.txt`